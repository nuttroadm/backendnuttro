"""
Sistema de IA com LangChain + LangGraph
Agentes especializados para nutriÃ§Ã£o
"""

from langchain_groq import ChatGroq
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import JsonOutputParser
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import add_messages
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Sequence
from pydantic import BaseModel, Field
import json
import os
import logging
import base64
from datetime import datetime, timezone
from pathlib import Path
from dotenv import load_dotenv

# Carregar .env
ROOT_DIR = Path(__file__).parent
load_dotenv(ROOT_DIR / '.env')

logger = logging.getLogger(__name__)

# ============================================
# CONFIGURAÃ‡ÃƒO - GROQ
# ============================================

# Groq API Key
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    logger.warning("âš ï¸ GROQ_API_KEY nÃ£o configurada no .env!")
else:
    logger.info(f"âœ… GROQ_API_KEY configurada (primeiros 10 chars: {GROQ_API_KEY[:10]}...)")

# Modelos Groq
# Modelos atualizados (llama-3.1-70b-versatile foi descontinuado)
GROQ_MODEL_TEXT = "llama-3.1-8b-instant"  # Modelo rÃ¡pido e eficiente para texto
GROQ_MODEL_VISION = "llama-3.2-11b-vision-preview"  # Modelo com suporte a anÃ¡lise de imagens

# ============================================
# STATE SCHEMAS
# ============================================

class AgentState(TypedDict):
    """Estado compartilhado entre os nÃ³s do grafo"""
    messages: Annotated[Sequence[BaseMessage], add_messages]
    context: Dict[str, Any]
    result: Optional[Dict[str, Any]]
    next_action: Optional[str]

class MealAnalysisResult(BaseModel):
    """Schema para anÃ¡lise de refeiÃ§Ã£o"""
    itens: List[str] = Field(description="Lista de alimentos identificados")
    porcoes: Dict[str, str] = Field(description="PorÃ§Ãµes estimadas")
    macros: Dict[str, float] = Field(description="Macronutrientes em gramas")
    calorias_estimadas: int = Field(description="Calorias totais estimadas")
    feedback: str = Field(description="Feedback motivador")
    sugestoes: List[str] = Field(description="SugestÃµes de melhoria")
    alinhamento_plano: str = Field(description="excelente, bom ou atencao")

class CheckInAnalysisResult(BaseModel):
    """Schema para anÃ¡lise de check-in"""
    pontos_fortes: List[str] = Field(description="O que estÃ¡ indo bem")
    areas_atencao: List[str] = Field(description="O que precisa melhorar")
    tendencias: Dict[str, str] = Field(description="TendÃªncias identificadas")
    sugestoes: List[str] = Field(description="SugestÃµes prÃ¡ticas")
    mensagem_motivacional: str = Field(description="Mensagem de apoio")
    alerta_nutricionista: bool = Field(description="Se deve alertar o nutricionista")
    motivo_alerta: Optional[str] = Field(description="Motivo do alerta se houver")

class ConsultaInsight(BaseModel):
    """Schema para insights de consulta"""
    resumo_progresso: str = Field(description="Resumo do progresso do paciente")
    principais_conquistas: List[str] = Field(description="Conquistas desde Ãºltima consulta")
    desafios_identificados: List[str] = Field(description="Desafios observados")
    recomendacoes_plano: List[str] = Field(description="SugestÃµes para o plano")
    metas_sugeridas: List[Dict[str, Any]] = Field(description="Metas sugeridas")
    pontos_atencao: List[str] = Field(description="Pontos que precisam de atenÃ§Ã£o")

# ============================================
# LLM FACTORY
# ============================================

def get_llm(model: str = None, temperature: float = 0.7) -> ChatGroq:
    """Factory para criar instÃ¢ncias do LLM usando Groq"""
    if not GROQ_API_KEY:
        logger.warning("âš ï¸ GROQ_API_KEY nÃ£o configurada!")
        return None
    
    return ChatGroq(
        model=model or GROQ_MODEL_TEXT,
        groq_api_key=GROQ_API_KEY,
        temperature=temperature
    )

# ============================================
# PROMPTS ESPECIALIZADOS
# ============================================

MEAL_ANALYSIS_PROMPT = """VocÃª Ã© Nuttro IA, um nutricionista virtual especializado em anÃ¡lise de refeiÃ§Ãµes.
Analise a refeiÃ§Ã£o descrita pelo usuÃ¡rio com MÃXIMA PRECISÃƒO e atenÃ§Ã£o aos detalhes.

CONTEXTO DO PACIENTE:
{patient_context}

PLANO ALIMENTAR ATUAL:
{meal_plan}

DESCRIÃ‡ÃƒO DA REFEIÃ‡ÃƒO FORNECIDA PELO USUÃRIO:
{descricao_refeicao}

INSTRUÃ‡Ã•ES CRÃTICAS:
1. Analise EXATAMENTE o que o usuÃ¡rio descreveu - nÃ£o invente alimentos que nÃ£o foram mencionados
2. Identifique TODOS os alimentos mencionados na descriÃ§Ã£o acima
3. Estime porÃ§Ãµes realistas baseado em tamanhos padrÃ£o brasileiros:
   - Arroz branco cozido: 1 concha mÃ©dia = ~150g = ~200 calorias, 45g carboidratos, 4g proteÃ­na
   - Ovo: 1 unidade mÃ©dia = ~60g = ~90 calorias, 6g proteÃ­na, 6g gordura
   - Frango grelhado: 1 filÃ© mÃ©dio = ~100g = ~165 calorias, 31g proteÃ­na, 3.6g gordura
   - FeijÃ£o cozido: 1 concha = ~100g = ~130 calorias, 8g proteÃ­na, 23g carboidratos
   - Salada: 1 prato raso = ~100g = ~20-50 calorias (dependendo do tipo)
4. Calcule macros em GRAMAS com precisÃ£o baseado nos alimentos identificados
5. Estime calorias totais somando todos os alimentos
6. Avalie alinhamento com o objetivo do paciente:
   - Se objetivo Ã© "perda de peso": verifique se estÃ¡ dentro das calorias recomendadas
   - Se objetivo Ã© "ganho de massa": verifique proteÃ­na suficiente
   - Se objetivo Ã© "manutenÃ§Ã£o": verifique equilÃ­brio nutricional
7. DÃª feedback ESPECÃFICO sobre a refeiÃ§Ã£o descrita, nÃ£o genÃ©rico
8. Sugira melhorias prÃ¡ticas baseadas no que foi realmente consumido

IMPORTANTE:
- NÃƒO invente alimentos que nÃ£o foram mencionados
- Se o usuÃ¡rio disse "arroz com ovo", analise APENAS arroz e ovo
- Use valores nutricionais mÃ©dios brasileiros para cada alimento
- Seja especÃ­fico: "Esta refeiÃ§Ã£o de arroz com ovo fornece..." nÃ£o "Uma refeiÃ§Ã£o balanceada..."
- Considere o objetivo do paciente ao avaliar
- Seja positivo mas realista

Responda APENAS em JSON vÃ¡lido, sem markdown, sem explicaÃ§Ãµes, com a estrutura:
{{
    "itens": ["lista EXATA de alimentos mencionados na descriÃ§Ã£o"],
    "porcoes": {{"alimento": "porÃ§Ã£o estimada (ex: 1 concha, 100g, 1 unidade)"}},
    "macros": {{"proteinas_g": 0.0, "carboidratos_g": 0.0, "gorduras_g": 0.0, "fibras_g": 0.0}},
    "calorias_estimadas": 0,
    "feedback": "feedback ESPECÃFICO sobre esta refeiÃ§Ã£o especÃ­fica, mencionando os alimentos reais",
    "sugestoes": ["sugestÃµes prÃ¡ticas baseadas nos alimentos realmente consumidos"],
    "alinhamento_plano": "excelente|bom|atencao"
}}"""

PATIENT_CHAT_PROMPT = """VocÃª Ã© Nuttro IA, um coach de nutriÃ§Ã£o virtual amigÃ¡vel e motivador.

CONTEXTO DO PACIENTE:
- Nome: {nome}
- Objetivo: {objetivo}
- Dias na jornada: {dias_jornada}
- NÃ­vel de adesÃ£o: {nivel_adesao}
- Ãšltima consulta: {ultima_consulta}
- Metas ativas: {metas}

HISTÃ“RICO RECENTE:
- Check-ins: {checkins_resumo}
- RefeiÃ§Ãµes: {refeicoes_resumo}

PERSONALIDADE:
- Seja empÃ¡tico, motivador e positivo
- Use linguagem clara e acessÃ­vel
- Celebre conquistas
- OfereÃ§a apoio em dificuldades
- DÃª sugestÃµes prÃ¡ticas
- Seja breve (mÃ¡ximo 3 parÃ¡grafos)

FUNÃ‡Ã•ES:
1. Responder dÃºvidas sobre nutriÃ§Ã£o
2. Motivar a seguir o plano
3. Oferecer sugestÃµes prÃ¡ticas
4. Fornecer apoio emocional
5. Analisar padrÃµes comportamentais

NÃƒO FAÃ‡A:
- DiagnÃ³sticos mÃ©dicos
- Promessas irrealistas
- CrÃ­ticas severas
- Substituir o nutricionista

Responda em portuguÃªs do Brasil."""

CHECKIN_ANALYSIS_PROMPT = """VocÃª Ã© um analista de dados nutricionais especializado.
Analise os check-ins do paciente para identificar padrÃµes e gerar insights.

DADOS DO PACIENTE:
{patient_info}

CHECK-INS RECENTES (Ãºltimos 7-30 dias):
{checkins_data}

METAS ATUAIS:
{metas}

Analise:
1. TendÃªncias nos indicadores (melhorando, estÃ¡vel, piorando)
2. PadrÃµes comportamentais
3. CorrelaÃ§Ãµes entre mÃ©tricas
4. Progresso em relaÃ§Ã£o Ã s metas
5. Se hÃ¡ sinais de alerta que precisam de atenÃ§Ã£o do nutricionista

Responda em JSON com:
{{
    "pontos_fortes": ["lista"],
    "areas_atencao": ["lista"],
    "tendencias": {{"metrica": "melhorando|estavel|piorando"}},
    "sugestoes": ["lista de sugestÃµes prÃ¡ticas"],
    "mensagem_motivacional": "mensagem de apoio",
    "alerta_nutricionista": true/false,
    "motivo_alerta": "motivo se houver"
}}"""

CONSULTA_INSIGHT_PROMPT = """VocÃª Ã© um assistente de IA especializado para nutricionistas.
Prepare insights DETALHADOS e ESPECÃFICOS para auxiliar na prÃ³xima consulta deste paciente especÃ­fico.

DADOS DO PACIENTE:
{patient_info}

HISTÃ“RICO DE CHECK-INS:
{checkins_summary}

HISTÃ“RICO DE REFEIÃ‡Ã•ES:
{meals_summary}

METAS E PROGRESSO:
{goals_progress}

ÃšLTIMA CONSULTA:
{last_consultation}

INSTRUÃ‡Ã•ES CRÃTICAS:
1. Analise os dados ESPECÃFICOS deste paciente - nÃ£o dÃª respostas genÃ©ricas
2. Identifique padrÃµes reais nos dados fornecidos (check-ins, refeiÃ§Ãµes, metas)
3. Compare o progresso atual com a Ãºltima consulta registrada
4. Seja especÃ­fico: mencione nÃºmeros, datas, tendÃªncias reais
5. Baseie suas recomendaÃ§Ãµes nos dados reais, nÃ£o em suposiÃ§Ãµes
6. Se nÃ£o houver dados suficientes, indique isso claramente

Gere insights ESPECÃFICOS para este paciente:
1. Resumo do progresso desde a Ãºltima consulta (com nÃºmeros e datas especÃ­ficas)
2. Principais conquistas do paciente (baseadas nos dados reais)
3. Desafios identificados nos dados (padrÃµes observados, nÃ£o genÃ©ricos)
4. RecomendaÃ§Ãµes para ajustar o plano (baseadas no objetivo e progresso real)
5. SugestÃµes de novas metas (alinhadas com o objetivo e histÃ³rico)
6. Pontos que precisam de atenÃ§Ã£o especial (alertas baseados em dados)

IMPORTANTE:
- NÃƒO use frases genÃ©ricas como "o paciente estÃ¡ progredindo bem"
- Use dados especÃ­ficos: "o paciente perdeu 2kg desde a Ãºltima consulta em [data]"
- Mencione alimentos especÃ­ficos que aparecem nas refeiÃ§Ãµes
- Cite mÃ©tricas reais dos check-ins
- Seja prÃ¡tico e acionÃ¡vel

Responda APENAS em JSON vÃ¡lido, sem markdown, com a estrutura:
{{
    "resumo_progresso": "resumo especÃ­fico com nÃºmeros e datas",
    "principais_conquistas": ["conquistas especÃ­ficas baseadas em dados reais"],
    "desafios_identificados": ["desafios especÃ­ficos observados nos dados"],
    "recomendacoes_plano": ["recomendaÃ§Ãµes prÃ¡ticas baseadas no objetivo e progresso"],
    "metas_sugeridas": [{{"meta": "nome", "prazo": "prazo", "motivo": "motivo baseado em dados"}}],
    "pontos_atencao": ["pontos especÃ­ficos que precisam de atenÃ§Ã£o"]
}}"""

# ============================================
# AGENTE DE ANÃLISE DE REFEIÃ‡Ã•ES
# ============================================

class MealAnalysisAgent:
    """Agente para anÃ¡lise de refeiÃ§Ãµes com visÃ£o"""
    
    def __init__(self):
        self.llm = get_llm(temperature=0.3)
        self.parser = JsonOutputParser(pydantic_object=MealAnalysisResult)
    
    async def _describe_image_with_vision(self, image_base64: str) -> str:
        """Usa modelo de visÃ£o do Groq para descrever a imagem de alimentos"""
        try:
            # Limpar base64 - remover prefixo data:image se presente
            if 'base64,' in image_base64:
                image_base64 = image_base64.split('base64,')[1]
            
            # Verificar se temos a API key
            if not GROQ_API_KEY:
                logger.warning("âš ï¸ GROQ_API_KEY nÃ£o configurada para anÃ¡lise de visÃ£o")
                return "Imagem nÃ£o pÃ´de ser analisada - API key nÃ£o configurada"
            
            # Usar modelo de visÃ£o do Groq
            try:
                vision_llm = ChatGroq(
                    model=GROQ_MODEL_VISION,
                    groq_api_key=GROQ_API_KEY,
                    temperature=0.3
                )
                
                # Criar mensagem com imagem no formato multimodal
                vision_prompt = """VocÃª Ã© um especialista em nutriÃ§Ã£o. Analise esta foto de refeiÃ§Ã£o e descreva:

1. TODOS os alimentos visÃ­veis na imagem (seja especÃ­fico: arroz branco, feijÃ£o preto, frango grelhado, etc.)
2. PorÃ§Ãµes aproximadas (pouco, mÃ©dio, bastante)
3. MÃ©todo de preparo visÃ­vel (grelhado, frito, cozido, cru)

Responda de forma direta e objetiva, listando os alimentos encontrados.
Exemplo: "Arroz branco (porÃ§Ã£o mÃ©dia), feijÃ£o carioca (1 concha), frango grelhado (1 filÃ© mÃ©dio), salada de alface e tomate"

Se nÃ£o conseguir identificar alimentos ou a imagem nÃ£o for de comida, diga claramente."""
                
                message = HumanMessage(content=[
                    {"type": "text", "text": vision_prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{image_base64}"
                        }
                    }
                ])
                
                logger.info(f"ðŸ” Analisando imagem com modelo de visÃ£o {GROQ_MODEL_VISION}...")
                response = await vision_llm.ainvoke([message])
                
                description = response.content.strip()
                logger.info(f"âœ… DescriÃ§Ã£o da imagem: {description[:200]}...")
                
                return description
                
            except Exception as e:
                logger.error(f"Erro ao usar modelo de visÃ£o Groq: {e}")
                import traceback
                logger.error(traceback.format_exc())
                return "NÃ£o foi possÃ­vel analisar a imagem automaticamente. Por favor, descreva os alimentos."
            
        except Exception as e:
            logger.error(f"Erro ao processar imagem: {e}")
            return "Erro ao processar imagem"
    
    async def analyze(
        self, 
        image_base64: str, 
        patient_context: Dict, 
        meal_plan: Dict = None,
        descricao: str = None
    ) -> Dict[str, Any]:
        """Analisa uma refeiÃ§Ã£o usando descriÃ§Ã£o textual ou imagem"""
        
        if not self.llm:
            return self._fallback_response()
        
        try:
            # Preparar contexto
            context_str = json.dumps(patient_context, ensure_ascii=False, indent=2) if patient_context else "Sem contexto do paciente"
            plan_str = json.dumps(meal_plan or {}, ensure_ascii=False, indent=2) if meal_plan else "Sem plano alimentar definido"
            
            # Priorizar descriÃ§Ã£o textual se fornecida
            if descricao and descricao.strip():
                descricao_refeicao = descricao.strip()
                logger.info(f"ðŸ“ Analisando refeiÃ§Ã£o por descriÃ§Ã£o textual: {descricao_refeicao[:100]}...")
            elif image_base64:
                # Gerar descriÃ§Ã£o da imagem (em produÃ§Ã£o, use uma API de visÃ£o real)
                descricao_refeicao = await self._describe_image_with_vision(image_base64)
                logger.info(f"ðŸ“· Analisando refeiÃ§Ã£o por imagem")
            else:
                logger.warning("âš ï¸ Nenhuma descriÃ§Ã£o ou imagem fornecida")
                return self._fallback_response()
            
            # Criar prompt completo com descriÃ§Ã£o
            prompt = MEAL_ANALYSIS_PROMPT.format(
                patient_context=context_str,
                meal_plan=plan_str,
                descricao_refeicao=descricao_refeicao
            )
            
            message = HumanMessage(content=prompt)
            response = await self.llm.ainvoke([message])
            
            # Parse da resposta
            content = response.content.strip()
            
            # Remover markdown se presente
            if content.startswith('```json'):
                content = content.replace('```json', '').replace('```', '').strip()
            elif content.startswith('```'):
                content = content.split('\n', 1)[1] if '\n' in content else content
                if content.endswith('```'):
                    content = content.rsplit('\n', 1)[0]
            
            result = json.loads(content)
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"Erro ao fazer parse JSON na anÃ¡lise de refeiÃ§Ã£o: {e}")
            logger.error(f"Resposta recebida: {response.content[:500] if 'response' in locals() else 'N/A'}")
            # Tentar extrair JSON da resposta
            try:
                import re
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                    return result
            except:
                pass
            return self._fallback_response()
        except Exception as e:
            logger.error(f"Erro na anÃ¡lise de refeiÃ§Ã£o: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self._fallback_response()
    
    def _fallback_response(self) -> Dict:
        return {
            "itens": ["NÃ£o foi possÃ­vel analisar"],
            "porcoes": {},
            "macros": {"proteinas_g": 0, "carboidratos_g": 0, "gorduras_g": 0, "fibras_g": 0},
            "calorias_estimadas": 0,
            "feedback": "NÃ£o consegui analisar esta imagem. Tente uma foto mais clara!",
            "sugestoes": ["Tire fotos com boa iluminaÃ§Ã£o"],
            "alinhamento_plano": "atencao"
        }

# ============================================
# AGENTE DE CHAT COM PACIENTE (LangGraph)
# ============================================

class PatientChatAgent:
    """Agente de chat usando LangGraph para conversas contextuais"""
    
    def __init__(self):
        # Temperature reduzida para respostas mais consistentes
        self.llm = get_llm(temperature=0.4)
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """ConstrÃ³i o grafo de conversaÃ§Ã£o"""
        
        # Definir nÃ³s
        async def understand_intent(state: AgentState) -> AgentState:
            """Entende a intenÃ§Ã£o do usuÃ¡rio"""
            # Analisar Ãºltima mensagem para classificar intenÃ§Ã£o
            last_msg = state["messages"][-1].content if state["messages"] else ""
            
            intents = ["duvida_nutricional", "motivacao", "relato_dificuldade", "duvida_plano", "outro"]
            # Por simplicidade, vamos direto para resposta
            state["next_action"] = "generate_response"
            return state
        
        async def generate_response(state: AgentState) -> AgentState:
            """Gera resposta contextualizada"""
            if not self.llm:
                state["result"] = {"response": "IA nÃ£o disponÃ­vel no momento."}
                return state
            
            try:
                context = state.get("context", {})
                
                system_prompt = PATIENT_CHAT_PROMPT.format(
                    nome=context.get("nome", "Paciente"),
                    objetivo=context.get("objetivo", "nÃ£o definido"),
                    dias_jornada=context.get("dias_jornada", 0),
                    nivel_adesao=context.get("nivel_adesao", "mÃ©dia"),
                    ultima_consulta=context.get("ultima_consulta", "nÃ£o registrada"),
                    metas=json.dumps(context.get("metas", []), ensure_ascii=False),
                    checkins_resumo=context.get("checkins_resumo", "sem dados"),
                    refeicoes_resumo=context.get("refeicoes_resumo", "sem dados")
                )
                
                messages = [SystemMessage(content=system_prompt)]
                
                # Adicionar histÃ³rico
                for msg in state["messages"]:
                    if isinstance(msg, HumanMessage):
                        messages.append(msg)
                    elif isinstance(msg, AIMessage):
                        messages.append(msg)
                
                response = await self.llm.ainvoke(messages)
                state["result"] = {"response": response.content}
                
            except Exception as e:
                logger.error(f"Erro no chat: {e}")
                state["result"] = {"response": "Desculpe, tive um problema. Tente novamente!"}
            
            return state
        
        # Construir grafo
        workflow = StateGraph(AgentState)
        
        # Adicionar nÃ³s
        workflow.add_node("understand", understand_intent)
        workflow.add_node("respond", generate_response)
        
        # Adicionar arestas
        workflow.add_edge(START, "understand")
        workflow.add_edge("understand", "respond")
        workflow.add_edge("respond", END)
        
        return workflow.compile()
    
    async def chat(
        self, 
        message: str, 
        patient_context: Dict, 
        chat_history: List[Dict] = None
    ) -> str:
        """Processa uma mensagem do paciente"""
        
        if not self.llm:
            return "Chat nÃ£o disponÃ­vel. Configure a API key."
        
        # Preparar histÃ³rico
        messages = []
        for msg in (chat_history or [])[-10:]:
            if msg.get("role") == "user":
                messages.append(HumanMessage(content=msg["content"]))
            else:
                messages.append(AIMessage(content=msg["content"]))
        
        # Adicionar mensagem atual
        messages.append(HumanMessage(content=message))
        
        # Estado inicial
        initial_state = {
            "messages": messages,
            "context": patient_context,
            "result": None,
            "next_action": None
        }
        
        # Executar grafo
        final_state = await self.graph.ainvoke(initial_state)
        
        return final_state.get("result", {}).get("response", "Erro ao processar mensagem.")

# ============================================
# AGENTE DE ANÃLISE DE CHECK-INS
# ============================================

class CheckInAnalysisAgent:
    """Agente para anÃ¡lise de padrÃµes em check-ins"""
    
    def __init__(self):
        self.llm = get_llm(temperature=0.4)
    
    async def analyze(
        self, 
        checkins: List[Dict], 
        patient_info: Dict, 
        metas: List[Dict] = None
    ) -> Dict[str, Any]:
        """Analisa padrÃµes nos check-ins"""
        
        if not self.llm or not checkins:
            return self._fallback_response()
        
        try:
            prompt = CHECKIN_ANALYSIS_PROMPT.format(
                patient_info=json.dumps(patient_info, ensure_ascii=False, indent=2),
                checkins_data=json.dumps(checkins, ensure_ascii=False, indent=2),
                metas=json.dumps(metas or [], ensure_ascii=False, indent=2)
            )
            
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            
            content = response.content.strip()
            if content.startswith('```'):
                content = content.split('\n', 1)[1]
                if content.endswith('```'):
                    content = content.rsplit('\n', 1)[0]
            
            return json.loads(content)
            
        except Exception as e:
            logger.error(f"Erro na anÃ¡lise de check-ins: {e}")
            return self._fallback_response()
    
    def _fallback_response(self) -> Dict:
        return {
            "pontos_fortes": ["Continue registrando seus check-ins!"],
            "areas_atencao": [],
            "tendencias": {},
            "sugestoes": ["Mantenha a constÃ¢ncia nos registros"],
            "mensagem_motivacional": "Cada dia Ã© uma nova oportunidade!",
            "alerta_nutricionista": False,
            "motivo_alerta": None
        }

# ============================================
# AGENTE DE INSIGHTS PARA CONSULTAS
# ============================================

class ConsultaInsightAgent:
    """Agente para gerar insights para o nutricionista"""
    
    def __init__(self):
        self.llm = get_llm(temperature=0.5)
    
    async def generate_insights(
        self,
        patient_info: Dict,
        checkins: List[Dict],
        meals: List[Dict],
        goals: List[Dict],
        last_consultation: Dict = None
    ) -> Dict[str, Any]:
        """Gera insights ESPECÃFICOS para auxiliar na consulta deste paciente"""
        
        if not self.llm:
            return self._fallback_response()
        
        try:
            # Preparar dados detalhados (nÃ£o apenas resumos)
            patient_info_str = json.dumps(patient_info, ensure_ascii=False, indent=2)
            
            # Resumir check-ins mas manter detalhes importantes
            checkins_summary = self._summarize_checkins(checkins)
            if checkins:
                # Adicionar informaÃ§Ãµes sobre os Ãºltimos check-ins
                ultimos_checkins = checkins[-5:] if len(checkins) > 5 else checkins
                checkins_summary += f"\n\nÃšltimos check-ins detalhados:\n{json.dumps(ultimos_checkins, ensure_ascii=False, indent=2)}"
            
            # Resumir refeiÃ§Ãµes mas manter detalhes das Ãºltimas
            meals_summary = self._summarize_meals(meals)
            if meals:
                # Adicionar informaÃ§Ãµes sobre as Ãºltimas refeiÃ§Ãµes
                ultimas_refeicoes = meals[-10:] if len(meals) > 10 else meals
                refeicoes_detalhadas = []
                for meal in ultimas_refeicoes:
                    refeicoes_detalhadas.append({
                        "data": meal.get("data"),
                        "itens": meal.get("itens", []),
                        "calorias": meal.get("calorias_estimadas"),
                        "alinhamento": meal.get("alinhamento_plano")
                    })
                meals_summary += f"\n\nÃšltimas refeiÃ§Ãµes detalhadas:\n{json.dumps(refeicoes_detalhadas, ensure_ascii=False, indent=2)}"
            
            # Resumir metas mas manter detalhes
            goals_progress = self._summarize_goals(goals)
            if goals:
                goals_progress += f"\n\nMetas detalhadas:\n{json.dumps(goals, ensure_ascii=False, indent=2)}"
            
            prompt = CONSULTA_INSIGHT_PROMPT.format(
                patient_info=patient_info_str,
                checkins_summary=checkins_summary,
                meals_summary=meals_summary,
                goals_progress=goals_progress,
                last_consultation=json.dumps(last_consultation or {}, ensure_ascii=False, indent=2)
            )
            
            logger.info(f"ðŸ“Š Gerando insights para paciente: {patient_info.get('nome', 'N/A')}")
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            
            content = response.content.strip()
            if content.startswith('```json'):
                content = content.replace('```json', '').replace('```', '').strip()
            elif content.startswith('```'):
                content = content.split('\n', 1)[1] if '\n' in content else content
                if content.endswith('```'):
                    content = content.rsplit('\n', 1)[0]
            
            result = json.loads(content)
            logger.info(f"âœ… Insights gerados com sucesso")
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"Erro ao fazer parse JSON nos insights: {e}")
            logger.error(f"Resposta recebida: {response.content[:500] if 'response' in locals() else 'N/A'}")
            return self._fallback_response()
        except Exception as e:
            logger.error(f"Erro ao gerar insights: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return self._fallback_response()
    
    def _summarize_checkins(self, checkins: List[Dict]) -> str:
        if not checkins:
            return "Sem check-ins registrados"
        
        # Calcular mÃ©dias
        metrics = ['consistencia_plano', 'frequencia_refeicoes', 'vegetais_frutas', 
                   'ingestao_liquido', 'energia_fisica', 'qualidade_sono']
        
        summary = {}
        for metric in metrics:
            values = [c.get(metric, 0) for c in checkins if c.get(metric)]
            if values:
                summary[metric] = round(sum(values) / len(values), 1)
        
        return json.dumps(summary, ensure_ascii=False)
    
    def _summarize_meals(self, meals: List[Dict]) -> str:
        if not meals:
            return "Sem refeiÃ§Ãµes registradas"
        
        total_calorias = sum(m.get('calorias_estimadas', 0) for m in meals)
        alinhamentos = [m.get('alinhamento_plano', 'atencao') for m in meals]
        
        return json.dumps({
            "total_refeicoes": len(meals),
            "calorias_media": round(total_calorias / len(meals)) if meals else 0,
            "alinhamento_excelente": alinhamentos.count('excelente'),
            "alinhamento_bom": alinhamentos.count('bom'),
            "alinhamento_atencao": alinhamentos.count('atencao')
        }, ensure_ascii=False)
    
    def _summarize_goals(self, goals: List[Dict]) -> str:
        if not goals:
            return "Sem metas definidas"
        
        return json.dumps({
            "total": len(goals),
            "ativas": len([g for g in goals if g.get('status') == 'ativa']),
            "concluidas": len([g for g in goals if g.get('status') == 'concluida']),
            "progresso_medio": round(sum(g.get('progresso_percentual', 0) for g in goals) / len(goals))
        }, ensure_ascii=False)
    
    def _fallback_response(self) -> Dict:
        return {
            "resumo_progresso": "Dados insuficientes para anÃ¡lise",
            "principais_conquistas": [],
            "desafios_identificados": [],
            "recomendacoes_plano": ["Coletar mais dados do paciente"],
            "metas_sugeridas": [],
            "pontos_atencao": []
        }

# ============================================
# FACTORY PARA AGENTES
# ============================================

_meal_agent = None
_chat_agent = None
_checkin_agent = None
_consulta_agent = None

def get_meal_analysis_agent() -> MealAnalysisAgent:
    global _meal_agent
    if _meal_agent is None:
        _meal_agent = MealAnalysisAgent()
    return _meal_agent

def get_patient_chat_agent() -> PatientChatAgent:
    global _chat_agent
    if _chat_agent is None:
        _chat_agent = PatientChatAgent()
    return _chat_agent

def get_checkin_analysis_agent() -> CheckInAnalysisAgent:
    global _checkin_agent
    if _checkin_agent is None:
        _checkin_agent = CheckInAnalysisAgent()
    return _checkin_agent

def get_consulta_insight_agent() -> ConsultaInsightAgent:
    global _consulta_agent
    if _consulta_agent is None:
        _consulta_agent = ConsultaInsightAgent()
    return _consulta_agent

# ============================================
# AGENTE DE GERAÃ‡ÃƒO DE PLANO ALIMENTAR
# ============================================

class MealPlanGenerationAgent:
    """Agente para gerar planos alimentares completos usando IA"""
    
    def __init__(self):
        self.llm = get_llm(temperature=0.7)
    
    async def generate_meal_plan(
        self,
        paciente_info: Dict,
        consulta_data: Dict,
        objetivo: str,
        restricoes: List[str] = None
    ) -> Dict[str, Any]:
        """Gera um plano alimentar completo e personalizado"""
        
        if not self.llm:
            logger.warning("âš ï¸ LLM nÃ£o disponÃ­vel, retornando plano bÃ¡sico")
            return self._fallback_response(objetivo)
        
        try:
            # Calcular IMC
            peso = consulta_data.get('avaliacao_fisica', {}).get('peso') or consulta_data.get('avaliacao_fisica', {}).get('peso_kg') or paciente_info.get('peso_atual_kg', 70)
            altura_cm = consulta_data.get('avaliacao_fisica', {}).get('altura') or consulta_data.get('avaliacao_fisica', {}).get('altura_cm') or paciente_info.get('altura_cm', 170)
            altura_m = altura_cm / 100 if altura_cm > 10 else altura_cm
            imc = peso / (altura_m ** 2) if altura_m > 0 else 0
            
            # Calcular idade
            data_nasc = paciente_info.get('data_nascimento')
            if data_nasc:
                try:
                    from datetime import datetime
                    nasc = datetime.fromisoformat(data_nasc.replace('Z', '+00:00'))
                    idade = (datetime.now(timezone.utc) - nasc).days // 365
                except:
                    idade = 30
            else:
                idade = 30
            
            # Preparar prompt
            prompt = MEAL_PLAN_GENERATION_PROMPT.format(
                nome=paciente_info.get('nome', 'Paciente'),
                idade=idade,
                peso_atual=peso,
                altura=altura_cm,
                imc=round(imc, 1),
                objetivo=objetivo,
                profissao=paciente_info.get('profissao', 'NÃ£o informado'),
                atividade_fisica=consulta_data.get('avaliacao_fisica', {}).get('atividade_fisica', 'SedentÃ¡ria'),
                restricoes=', '.join(restricoes) if restricoes else 'Nenhuma',
                historico_saude=json.dumps(consulta_data.get('anamnese', {}), ensure_ascii=False),
                anamnese=json.dumps(consulta_data.get('anamnese', {}), ensure_ascii=False, indent=2),
                avaliacao_fisica=json.dumps(consulta_data.get('avaliacao_fisica', {}), ensure_ascii=False, indent=2),
                avaliacao_emocional=json.dumps(consulta_data.get('avaliacao_emocional', {}), ensure_ascii=False, indent=2),
                avaliacao_comportamental=json.dumps(consulta_data.get('avaliacao_comportamental', {}), ensure_ascii=False, indent=2),
                avaliacao_bem_estar=json.dumps(consulta_data.get('avaliacao_bem_estar', {}), ensure_ascii=False, indent=2)
            )
            
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])
            
            content = response.content.strip()
            # Limpar markdown code blocks se houver
            if content.startswith('```'):
                lines = content.split('\n')
                content = '\n'.join(lines[1:-1]) if content.endswith('```') else '\n'.join(lines[1:])
            
            result = json.loads(content)
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"Erro ao parsear JSON da IA: {e}")
            logger.error(f"Resposta recebida: {content[:500]}")
            return self._fallback_response(objetivo)
        except Exception as e:
            logger.error(f"Erro ao gerar plano alimentar: {e}")
            return self._fallback_response(objetivo)
    
    def _fallback_response(self, objetivo: str) -> Dict:
        """Resposta fallback caso a IA nÃ£o esteja disponÃ­vel"""
        return {
            "calorias_diarias": 1800,
            "macros": {
                "proteinas_g": 135,
                "carboidratos_g": 180,
                "gorduras_g": 60,
                "fibras_g": 25
            },
            "refeicoes": [
                {
                    "tipo": "CafÃ© da manhÃ£",
                    "horario": "07:00",
                    "alimentos": [
                        {"nome": "Aveia", "quantidade": "40g", "calorias": 150},
                        {"nome": "Banana", "quantidade": "1 unidade mÃ©dia", "calorias": 90},
                        {"nome": "Leite desnatado", "quantidade": "200ml", "calorias": 70}
                    ],
                    "total_calorias": 310,
                    "observacoes": "Preparar com Ã¡gua morna"
                }
            ],
            "orientacoes_gerais": [
                "Beba pelo menos 2 litros de Ã¡gua por dia",
                "FaÃ§a as refeiÃ§Ãµes em horÃ¡rios regulares",
                "Mastigue bem os alimentos"
            ],
            "hidratacao": {
                "quantidade_ml": 2000,
                "dicas": ["Beba Ã¡gua ao longo do dia", "Evite beber durante as refeiÃ§Ãµes"]
            },
            "suplementacao": {
                "recomendada": False,
                "itens": []
            },
            "proxima_consulta_dias": 15,
            "proxima_consulta_justificativa": "Acompanhamento inicial para avaliar adesÃ£o ao plano"
        }

_meal_plan_agent = None

def get_meal_plan_agent() -> MealPlanGenerationAgent:
    global _meal_plan_agent
    if _meal_plan_agent is None:
        _meal_plan_agent = MealPlanGenerationAgent()
    return _meal_plan_agent

